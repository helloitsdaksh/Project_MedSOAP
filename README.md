# ğŸš€ Fine-Tuning and Evaluating Unsloth-Based LLM

This repository provides a streamlined pipeline for **fine-tuning** and **evaluating** a **LoRA-adapted Unsloth model** using **TRL's SFTTrainer** and **Hugging Face Transformers**. It supports **Docker-based deployment** for easy execution in isolated environments.

find the model and its checkpoints here: <https://drive.google.com/drive/folders/1FhOqbE5cm_P7sIVGea2N-xnxru3kjV7P?usp=drive_link>

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Dockerfile              # Docker setup for containerized execution
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ trainer.py              # Training script for fine-tuning the model
â”œâ”€â”€ evaluation.py           # Evaluation script with BLEU, ROUGE, and METEOR scores
â”œâ”€â”€ functions.py            # Utility functions for training and evaluation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_llama_formatted.csv         # Training dataset
â”‚   â”œâ”€â”€ validation_llama_formatted.csv    # Validation dataset
â”‚   â”œâ”€â”€ test_llama_formatted.csv          # Test dataset
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lora_model/       # Fine-tuned LoRA model (output)
â”œâ”€â”€ generated_output.txt   # Output from the model
â”œâ”€â”€ evaluation_results.json # Metrics from evaluation
â””â”€â”€ README.md              # Documentation
```

Hereâ€™s a detailed README with clear sections on how to train the model from scratch or use the FastAPI-based inference with Docker.

ğŸš€ Fine-Tuning & Serving Unsloth LLM with FastAPI

This repository provides a streamlined workflow for:
1.	Fine-tuning a LoRA-adapted Unsloth model using TRLâ€™s SFTTrainer and Hugging Face Transformers.
2.	Deploying the model via a FastAPI server, enabling real-time inference.
3.	Running everything inside a Docker container for portability.

# ğŸ“Œ Choose Your Approach
### Option 1: Fine-tune the model from scratch (Trainer & Evaluation Section).
### Option 2: Use Docker & FastAPI for inference (API Section).



# ğŸ“Œ Option 1: Fine-Tuning the Model from Scratch

ğŸ›  Installation & Setup

1ï¸âƒ£ Install Dependencies

`pip install -r requirements.txt`

2ï¸âƒ£ Prepare Data

Ensure your training, validation, and test datasets are stored in the data/ directory and formatted as required.

ğŸ‹ï¸ Fine-Tuning the Model

Run Trainer Script

`python trainer.py --train_data "data/train_llama_formatted.csv" --valid_data "data/validation_llama_formatted.csv"`

	*	This fine-tunes the LoRA-adapted Unsloth model.
	*	The trained model will be saved inside the models/lora_model/ directory.

ğŸ“Š Evaluating the Fine-Tuned Model

Run Evaluation Script

`python evaluation.py --model_path "models/lora_model" --test_data "data/test_llama_formatted.csv"`

	*	This script computes BLEU, ROUGE, and METEOR scores for evaluating model performance.
	*	The evaluation results are saved in evaluation_results.json.

# ğŸ“Œ Option 2: Running the API with Docker

ğŸš€ Deploy Model Using FastAPI

Once the model is trained or downloaded, you can serve it via FastAPI.

1ï¸âƒ£ Build the Docker Image

`docker build -t MedSOAP .`

2ï¸âƒ£ Run the API in a Docker Container

`docker run --gpus all -p 8000:8000 -it unsloth-llm-api`

	*	The API will be accessible at: http://localhost:8000

ğŸ“¡ API Endpoints

1ï¸âƒ£ Health Check
*	Check if the API is running and detect device (CPU/GPU).
*	Endpoint: GET /

*	Response:
```json
 {
    "status": "API is running",
    "device": "cuda"
 }
```


2ï¸âƒ£ Generate Text
*	Generate a response from the fine-tuned LLM model.
*	Endpoint: POST /generate

*	Request Body (JSON):

```json
{
"text": "Doctor: Hello, can you please tell me about your past medical history?",
"max_tokens": 2048
}
```

*	Response (JSON):
```json
{
    "input": "Doctor: Hello, can you please tell me about your past medical history?",
    "generated_text": "The patient reports a history of..."
}
```


# ğŸ“Œ How to Test the API

1ï¸âƒ£ Using curl
```
curl -X 'POST' \
'http://localhost:8000/generate' \
-H 'Content-Type: application/json' \
-d '{"text": "Doctor: What brings you in today?", "max_tokens": 2048}'
```

2ï¸âƒ£ Using Python (requests)
```python
import requests

url = "http://localhost:8000/generate"
payload = {"text": "Doctor: What brings you in today?", "max_tokens": 512}

response = requests.post(url, json=payload)
print(response.json())

```



# ğŸ“Œ Notes
*	If you want to fine-tune the model, use Option 1.
*	If you just need to run inference, use Docker and FastAPI (Option 2).
*	Ensure you have an NVIDIA GPU for optimal performance.
*	Modify trainer.py to adjust LoRA hyperparameters as needed.
*	The dataset format should align with functions.py preprocessing logic.

# ğŸ’¡ Future Work
*	Implement a streaming inference API for longer texts.
*	Optimize memory usage & add CPU support for macOS Metal.

ğŸ“ License

This project is open-source under the MIT License.

ğŸš€ Happy fine-tuning & inferencing! ğŸš€
